{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9e9a0b",
   "metadata": {},
   "source": [
    "# Llama 3.1 8B — QLoRA fine-tune on Abstract → Summary pairs\n",
    "- Input: `data/abstract_pairs.parquet` (paper_id, title, input_text, summary, source)\n",
    "- Model: `meta-llama/Meta-Llama-3.1-8B` in 4-bit (QLoRA)\n",
    "- Goal: teach the model to summarize abstracts (100–200 words)\n",
    "- We will:\n",
    "  1. Load and format data\n",
    "  2. Fine-tune with LoRA (PEFT + TRL)\n",
    "  3. Save adapter\n",
    "  4. Reload base vs fine-tuned and compare with ROUGE-L & BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccac0ca",
   "metadata": {},
   "source": [
    "## Ensure GPU is Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc8e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name() if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe609908",
   "metadata": {},
   "source": [
    "## 1) Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2953757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Fri Dec  5 13:18:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   32C    P8             39W /  390W |    8383MiB /  24576MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4252      G   /usr/lib/xorg/Xorg                      201MiB |\n",
      "|    0   N/A  N/A            4481      G   /usr/bin/gnome-shell                     26MiB |\n",
      "|    0   N/A  N/A            5196      G   .../7423/usr/lib/firefox/firefox        167MiB |\n",
      "|    0   N/A  N/A            9625      G   /usr/bin/nautilus                        12MiB |\n",
      "|    0   N/A  N/A           10763      C   ...earchPaperFineTune/bin/python       7884MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os, random, pandas as pd, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "HF_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "OUT_DIR = Path(\"outputs/llama31_8b_qlora_abstracts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "!nvidia-smi\n",
    "\n",
    "# If you haven't logged into Hugging Face on this machine, run once:\n",
    "# from huggingface_hub import login\n",
    "# login()  # paste your HF token (must have Llama 3.1 access)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d657838",
   "metadata": {},
   "source": [
    "## 2) Load dataset (abstract pairs) and build prompts\n",
    "Expects: `data/abstract_pairs.parquet` with columns:\n",
    "- `paper_id`\n",
    "- `title`\n",
    "- `input_text` (abstract)\n",
    "- `summary` (target summary)\n",
    "- `source`  (s2/gpt/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb7b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['paper_id', 'title', 'input_text', 'summary', 'source']\n",
      "Total rows: 6800\n",
      "Using rows: 6800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>input_text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Observations on LLMs for Telecom Domain: Capab...</td>\n",
       "      <td>The landscape for building conversational inte...</td>\n",
       "      <td>Recent advancements in artificial intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Connect the dots: Dataset Condensation, Differ...</td>\n",
       "      <td>Our work focuses on understanding the underpin...</td>\n",
       "      <td>This research explores how to improve a proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grounding Language about Belief in a Bayesian ...</td>\n",
       "      <td>Despite the fact that beliefs are mental state...</td>\n",
       "      <td>Humans often discuss each other's beliefs, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open-world Semi-supervised Novel Class Discovery</td>\n",
       "      <td>Traditional semi-supervised learning tasks ass...</td>\n",
       "      <td>In many real-world situations, we encounter da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Understanding Survey Paper Taxonomy about Larg...</td>\n",
       "      <td>As new research on Large Language Models (LLMs...</td>\n",
       "      <td>As research on Large Language Models (LLMs) gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Restore Anything Pipeline: Segment Anything Me...</td>\n",
       "      <td>Recent image restoration methods have produced...</td>\n",
       "      <td>Recent advancements in image restoration have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Observations on LLMs for Telecom Domain: Capab...   \n",
       "1  Connect the dots: Dataset Condensation, Differ...   \n",
       "2  Grounding Language about Belief in a Bayesian ...   \n",
       "3   Open-world Semi-supervised Novel Class Discovery   \n",
       "4  Understanding Survey Paper Taxonomy about Larg...   \n",
       "5  Restore Anything Pipeline: Segment Anything Me...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  The landscape for building conversational inte...   \n",
       "1  Our work focuses on understanding the underpin...   \n",
       "2  Despite the fact that beliefs are mental state...   \n",
       "3  Traditional semi-supervised learning tasks ass...   \n",
       "4  As new research on Large Language Models (LLMs...   \n",
       "5  Recent image restoration methods have produced...   \n",
       "\n",
       "                                             summary  \n",
       "0  Recent advancements in artificial intelligence...  \n",
       "1  This research explores how to improve a proces...  \n",
       "2  Humans often discuss each other's beliefs, eve...  \n",
       "3  In many real-world situations, we encounter da...  \n",
       "4  As research on Large Language Models (LLMs) gr...  \n",
       "5  Recent advancements in image restoration have ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/abstract_pairs.parquet\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Total rows:\", len(df))\n",
    "\n",
    "# Keep up to 6800 examples\n",
    "if len(df) > 6800:\n",
    "    df = df.sample(6800, random_state=SEED).reset_index(drop=True)\n",
    "print(\"Using rows:\", len(df))\n",
    "\n",
    "df[[\"title\", \"input_text\", \"summary\"]].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da1ca7",
   "metadata": {},
   "source": [
    "### Build supervised prompts\n",
    "We create a single text field that includes:\n",
    "Instruction + Title + Abstract + `Output:` + gold summary\n",
    "\n",
    "The model is trained with next-token prediction on this sequence (SFT style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ffe3ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "763   ### Instruction:\\nSummarize the following abst...   \n",
      "373   ### Instruction:\\nSummarize the following abst...   \n",
      "4836  ### Instruction:\\nSummarize the following abst...   \n",
      "2629  ### Instruction:\\nSummarize the following abst...   \n",
      "308   ### Instruction:\\nSummarize the following abst...   \n",
      "4100  ### Instruction:\\nSummarize the following abst...   \n",
      "\n",
      "                                                  title  \\\n",
      "763   Optimizing fairness tradeoffs in machine learn...   \n",
      "373   Intuition emerges in Maximum Caliber models at...   \n",
      "4836  Learning Computational Efficient Bots with Cos...   \n",
      "2629  A Nonlinear Hash-based Optimization Method for...   \n",
      "308   Boosting Theory-of-Mind Performance in Large L...   \n",
      "4100  Data Formulator: AI-powered Concept-driven Vis...   \n",
      "\n",
      "                                             input_text  \\\n",
      "763   Improving the fairness of machine learning mod...   \n",
      "373   Whether large predictive models merely parrot ...   \n",
      "4836  Deep reinforcement learning (DRL) techniques h...   \n",
      "2629  Sparse matrix-vector multiplication (SpMV) is ...   \n",
      "308   Large language models (LLMs) excel in many tas...   \n",
      "4100  With most modern visualization tools, authors ...   \n",
      "\n",
      "                                                summary  \n",
      "763   Researchers are working to make machine learni...  \n",
      "373   Researchers have explored how large predictive...  \n",
      "4836  Researchers have developed a new approach to i...  \n",
      "2629  Sparse matrix-vector multiplication (SpMV) is ...  \n",
      "308   A recent study explored how to improve the abi...  \n",
      "4100  Data Formulator is a new tool designed to make...  \n"
     ]
    }
   ],
   "source": [
    "def build_prompt(row):\n",
    "    return (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Summarize the following abstract into a clear, faithful 100–200 word summary for a general audience.\\n\\n\"\n",
    "        f\"### Title:\\n{row['title']}\\n\\n\"\n",
    "        f\"### Abstract:\\n{row['input_text']}\\n\\n\"\n",
    "        \"### Output:\\n\"\n",
    "        f\"{row['summary']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"text\"] = df.apply(build_prompt, axis=1)\n",
    "\n",
    "# Split into train/validation\n",
    "train_df, val_df = train_test_split(df[[\"text\", \"title\", \"input_text\", \"summary\"]],\n",
    "                                    test_size=0.1, random_state=SEED)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True)[[\"text\"]])\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True)[[\"text\"]])\n",
    "\n",
    "len(train_ds), len(val_ds)\n",
    "\n",
    "print(train_df.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c599b1",
   "metadata": {},
   "source": [
    "## 3) Tokenizer & model (4-bit with BitsAndBytes for QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d157ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8404295d11146ddbc629b8556731ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "SEQ_LEN = 4096  # safe starting point for RTX 3090 + 4-bit\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(HF_MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # good on Ampere (3090)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "model.config.use_cache = False  # for gradient checkpointing\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e888f3",
   "metadata": {},
   "source": [
    "## 4) LoRA config + SFTTrainer setup\n",
    "We use TRL's SFTTrainer to:\n",
    "- apply LoRA (PEFT)\n",
    "- handle packing and training loop\n",
    "- log loss / eval metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f12e721-41f2-4260-ae52-4202040215a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae8c4994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d56b995595c4b149fa410df379c9eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0b79306abb42d195d0e6516b0e25ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16fb8491c5b417299ada0e70f9f08f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a0e94f302349829119a18ad1caf791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef01cf29cff746a6b46894d886d64cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e179a6859e7a41e181d50766c351e8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import TrainingArguments  # still needed because SFTConfig inherits from it\n",
    "\n",
    "# LoRA config (same as before)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# All trainer hyperparameters now go into SFTConfig\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,      # effective batch size = 8\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=2,                 # start with 1 for smoke test, then 2–3\n",
    "    bf16=True,                          # or set to False/use fp16 if bf16 fails\n",
    "    logging_steps=25,                   # print train loss every 25 steps\n",
    "\n",
    "    # eval/save strategy names in recent transformers\n",
    "    eval_strategy=\"steps\",              # <— NOT evaluation_strategy\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",                   # change to \"wandb\" if you use W&B\n",
    "\n",
    "    # SFT-specific bits\n",
    "    dataset_text_field=\"text\",          # column name in train_ds / val_ds\n",
    "    max_length=SEQ_LEN,                 # <— use max_length, not max_seq_length\n",
    "    packing=False,                       # pack multiple examples per sequence\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,        # pass the SFTConfig here\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tok,   # tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f927f1",
   "metadata": {},
   "source": [
    "## 5) Train and monitor progress\n",
    "Watch `loss` in the logs. If you enabled W&B, you'll get a live dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5b51e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n",
      "/home/leo/anaconda3/envs/ResearchPaperFineTune/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1530' max='1530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1530/1530 2:28:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.479800</td>\n",
       "      <td>1.480633</td>\n",
       "      <td>1.491614</td>\n",
       "      <td>933542.000000</td>\n",
       "      <td>0.638644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.464800</td>\n",
       "      <td>1.463040</td>\n",
       "      <td>1.471080</td>\n",
       "      <td>1864268.000000</td>\n",
       "      <td>0.642976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.467100</td>\n",
       "      <td>1.447510</td>\n",
       "      <td>1.449367</td>\n",
       "      <td>2799763.000000</td>\n",
       "      <td>0.647009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.339300</td>\n",
       "      <td>1.449603</td>\n",
       "      <td>1.378876</td>\n",
       "      <td>3734780.000000</td>\n",
       "      <td>0.648484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.298000</td>\n",
       "      <td>1.445136</td>\n",
       "      <td>1.349375</td>\n",
       "      <td>4662373.000000</td>\n",
       "      <td>0.649818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.343600</td>\n",
       "      <td>1.442196</td>\n",
       "      <td>1.353186</td>\n",
       "      <td>5597056.000000</td>\n",
       "      <td>0.650690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/envs/ResearchPaperFineTune/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/leo/anaconda3/envs/ResearchPaperFineTune/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/leo/anaconda3/envs/ResearchPaperFineTune/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1530, training_loss=1.4128860367669, metrics={'train_runtime': 8933.9005, 'train_samples_per_second': 1.37, 'train_steps_per_second': 0.171, 'total_flos': 2.5855568781312e+17, 'train_loss': 1.4128860367669, 'entropy': 1.324153670668602, 'num_tokens': 5710000.0, 'mean_token_accuracy': 0.6751571699976922, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb71273",
   "metadata": {},
   "source": [
    "## 6) Save the LoRA adapter + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7960319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter + tokenizer saved to: outputs/llama31_8b_qlora_abstracts\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"Adapter + tokenizer saved to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6bd76",
   "metadata": {},
   "source": [
    "## 7) Reload base vs fine-tuned and compare on a held-out subset\n",
    "We will:\n",
    "- Take ~200 examples from `val_df`\n",
    "- Prompt both base and fine-tuned models to summarize the abstract\n",
    "- Compute ROUGE-L and BERTScore-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "855cce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554efc632edd4c48ab01e47499b939b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b69c4c8e07049ad89260ff88af9d23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval examples: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8bc26e00c742bfa484c27712184f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c492abe5b2454ebc99b6071e15d016b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93965e4d58c6437ba1ad9431b327240b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109c02a901634354847b13fb6fb03a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b75da4a79f14186bcfe15a92e73c828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189c510f976b424eaa03d0f92c60c6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccae4ab44a44df68629b4626adb78c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60156cff8a7443b6b230ce6d0316a861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROUGE-L ===\n",
      "Base      : 0.04510833292555738\n",
      "Fine-tuned: 0.3925438207974889\n",
      "\n",
      "=== BERTScore-F1 (mean) ===\n",
      "Base      : -3.84653377532959\n",
      "Fine-tuned: 0.5303022861480713\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import bert_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reload base model (same 4-bit config)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model.config.use_cache = True\n",
    "\n",
    "# Reload fine-tuned: base + LoRA adapter\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(ft_model, OUT_DIR)\n",
    "ft_model.config.use_cache = True\n",
    "\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tok, device_map=\"auto\")\n",
    "ft_pipe   = pipeline(\"text-generation\", model=ft_model,  tokenizer=tok, device_map=\"auto\")\n",
    "\n",
    "# Small validation subset for speed\n",
    "VAL_N = min(200, len(val_df))\n",
    "eval_slice = val_df.sample(VAL_N, random_state=SEED).reset_index(drop=True)\n",
    "print(\"Eval examples:\", len(eval_slice))\n",
    "\n",
    "def make_eval_prompt(abstract_text: str, title: str | None = None,\n",
    "                     min_w: int = 100, max_w: int = 200) -> str:\n",
    "    title_str = f\"Title: {title}\\n\\n\" if title else \"\"\n",
    "    return (\n",
    "        f\"Summarize the following abstract into a clear, faithful {min_w}–{max_w} word summary for a general audience.\\n\\n\"\n",
    "        f\"{title_str}\"\n",
    "        f\"Abstract:\\n{abstract_text}\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_summary(pipe, prompt: str, max_new: int = 300) -> str:\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "    )[0][\"generated_text\"]\n",
    "    # If model echoes the prompt, strip everything before \"Summary:\"\n",
    "    if \"Summary:\" in out:\n",
    "        out = out.split(\"Summary:\", 1)[-1]\n",
    "    return out.strip()\n",
    "\n",
    "\n",
    "refs = []\n",
    "base_preds = []\n",
    "ft_preds = []\n",
    "\n",
    "for _, row in tqdm(eval_slice.iterrows(), total=len(eval_slice), desc=\"Evaluating\"):\n",
    "    prompt = make_eval_prompt(row[\"input_text\"], row[\"title\"])\n",
    "    refs.append(row[\"summary\"])\n",
    "    base_preds.append(generate_summary(base_pipe, prompt))\n",
    "    ft_preds.append(generate_summary(ft_pipe, prompt))\n",
    "\n",
    "# ROUGE-L\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rg_base = rouge.compute(predictions=base_preds, references=refs, rouge_types=[\"rougeL\"])\n",
    "rg_ft   = rouge.compute(predictions=ft_preds,   references=refs, rouge_types=[\"rougeL\"])\n",
    "\n",
    "# BERTScore (F1)\n",
    "P_b, R_b, F_b = bert_score.score(base_preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "P_f, R_f, F_f = bert_score.score(ft_preds,   refs, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "print(\"\\n=== ROUGE-L ===\")\n",
    "print(\"Base      :\", rg_base[\"rougeL\"])\n",
    "print(\"Fine-tuned:\", rg_ft[\"rougeL\"])\n",
    "\n",
    "print(\"\\n=== BERTScore-F1 (mean) ===\")\n",
    "print(\"Base      :\", float(F_b.mean()))\n",
    "print(\"Fine-tuned:\", float(F_f.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eae6f9",
   "metadata": {},
   "source": [
    "## 8) Helper: load fine-tuned pipeline later and run a test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4fe722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b47255e994545d480380fdf5367e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuned model summary ---\n",
      "\n",
      "Summarize the following abstract into a clear, faithful 100–200 word summary for a general audience.\n",
      "\n",
      "Title: Efficient Sparse Attention for Long-Context Transformers\n",
      "\n",
      "Abstract:\n",
      "Transformers use self-attention to capture long-range dependencies in sequences.\n",
      "However, the quadratic complexity of standard attention limits practicality for very long inputs.\n",
      "We propose a sparse attention mechanism that preserves performance while reducing computational cost,\n",
      "and demonstrate improvements on language modeling and long-document summarization benchmarks.\n",
      "\n",
      "Summary: Researchers have developed a new method called sparse attention to improve how computers understand and process long texts, like articles or books. Traditional methods, known as self-attention, are effective but require a lot of computing power, making them impractical for very long documents. The new sparse attention method keeps the quality of the text analysis high while using less computing resources. This is important because it allows for better language processing in applications like summarizing long documents or improving chatbots. The researchers tested their method on various tasks and found that it performed well, suggesting that this approach could make advanced language processing more accessible and efficient.\n"
     ]
    }
   ],
   "source": [
    "def load_finetuned_pipeline(adapter_dir: str | Path = OUT_DIR):\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        HF_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    ft = PeftModel.from_pretrained(base, adapter_dir)\n",
    "    ft.config.use_cache = True\n",
    "    return pipeline(\"text-generation\", model=ft, tokenizer=tok, device_map=\"auto\")\n",
    "\n",
    "\n",
    "ft_pipe = load_finetuned_pipeline()\n",
    "\n",
    "test_abstract = \"\"\"Transformers use self-attention to capture long-range dependencies in sequences.\n",
    "However, the quadratic complexity of standard attention limits practicality for very long inputs.\n",
    "We propose a sparse attention mechanism that preserves performance while reducing computational cost,\n",
    "and demonstrate improvements on language modeling and long-document summarization benchmarks.\"\"\"\n",
    "\n",
    "test_title = \"Efficient Sparse Attention for Long-Context Transformers\"\n",
    "\n",
    "test_prompt = make_eval_prompt(test_abstract, test_title)\n",
    "print(\"\\n--- Fine-tuned model summary ---\\n\")\n",
    "print(ft_pipe(test_prompt, max_new_tokens=280, do_sample=False, temperature=0.0)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858425b",
   "metadata": {},
   "source": [
    "## 9) (Optional) Merge LoRA into full weights (for export)\n",
    "This step is optional and more VRAM-heavy. Usually you can just keep LoRA separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "# merged = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     OUT_DIR,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# ).merge_and_unload()\n",
    "# merged_dir = OUT_DIR / \"merged_fp16\"\n",
    "# merged.save_pretrained(merged_dir)\n",
    "# tok.save_pretrained(merged_dir)\n",
    "# print(\"Merged full model saved to:\", merged_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchPaperFineTune",
   "language": "python",
   "name": "researchpaperfinetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
