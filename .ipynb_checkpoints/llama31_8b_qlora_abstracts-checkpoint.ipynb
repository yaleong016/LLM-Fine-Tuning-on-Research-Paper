{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9e9a0b",
   "metadata": {},
   "source": [
    "# Llama 3.1 8B — QLoRA fine-tune on Abstract → Summary pairs\n",
    "- Input: `data/abstract_pairs.parquet` (paper_id, title, input_text, summary, source)\n",
    "- Model: `meta-llama/Meta-Llama-3.1-8B` in 4-bit (QLoRA)\n",
    "- Goal: teach the model to summarize abstracts (100–200 words)\n",
    "- We will:\n",
    "  1. Load and format data\n",
    "  2. Fine-tune with LoRA (PEFT + TRL)\n",
    "  3. Save adapter\n",
    "  4. Reload base vs fine-tuned and compare with ROUGE-L & BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccac0ca",
   "metadata": {},
   "source": [
    "## Ensure GPU is Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc8e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name() if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe609908",
   "metadata": {},
   "source": [
    "## 1) Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2953757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Fri Dec  5 12:21:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   34C    P8             34W /  390W |     504MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4252      G   /usr/lib/xorg/Xorg                      199MiB |\n",
      "|    0   N/A  N/A            4481      G   /usr/bin/gnome-shell                     31MiB |\n",
      "|    0   N/A  N/A            5196      G   .../7423/usr/lib/firefox/firefox        175MiB |\n",
      "|    0   N/A  N/A            9625      G   /usr/bin/nautilus                        12MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os, random, pandas as pd, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "HF_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "OUT_DIR = Path(\"outputs/llama31_8b_qlora_abstracts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "!nvidia-smi\n",
    "\n",
    "# If you haven't logged into Hugging Face on this machine, run once:\n",
    "# from huggingface_hub import login\n",
    "# login()  # paste your HF token (must have Llama 3.1 access)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d657838",
   "metadata": {},
   "source": [
    "## 2) Load dataset (abstract pairs) and build prompts\n",
    "Expects: `data/abstract_pairs.parquet` with columns:\n",
    "- `paper_id`\n",
    "- `title`\n",
    "- `input_text` (abstract)\n",
    "- `summary` (target summary)\n",
    "- `source`  (s2/gpt/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb7b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['paper_id', 'title', 'input_text', 'summary', 'source']\n",
      "Total rows: 6800\n",
      "Using rows: 6800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>input_text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Observations on LLMs for Telecom Domain: Capab...</td>\n",
       "      <td>The landscape for building conversational inte...</td>\n",
       "      <td>Recent advancements in artificial intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Connect the dots: Dataset Condensation, Differ...</td>\n",
       "      <td>Our work focuses on understanding the underpin...</td>\n",
       "      <td>This research explores how to improve a proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grounding Language about Belief in a Bayesian ...</td>\n",
       "      <td>Despite the fact that beliefs are mental state...</td>\n",
       "      <td>Humans often discuss each other's beliefs, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open-world Semi-supervised Novel Class Discovery</td>\n",
       "      <td>Traditional semi-supervised learning tasks ass...</td>\n",
       "      <td>In many real-world situations, we encounter da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Understanding Survey Paper Taxonomy about Larg...</td>\n",
       "      <td>As new research on Large Language Models (LLMs...</td>\n",
       "      <td>As research on Large Language Models (LLMs) gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Restore Anything Pipeline: Segment Anything Me...</td>\n",
       "      <td>Recent image restoration methods have produced...</td>\n",
       "      <td>Recent advancements in image restoration have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Observations on LLMs for Telecom Domain: Capab...   \n",
       "1  Connect the dots: Dataset Condensation, Differ...   \n",
       "2  Grounding Language about Belief in a Bayesian ...   \n",
       "3   Open-world Semi-supervised Novel Class Discovery   \n",
       "4  Understanding Survey Paper Taxonomy about Larg...   \n",
       "5  Restore Anything Pipeline: Segment Anything Me...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  The landscape for building conversational inte...   \n",
       "1  Our work focuses on understanding the underpin...   \n",
       "2  Despite the fact that beliefs are mental state...   \n",
       "3  Traditional semi-supervised learning tasks ass...   \n",
       "4  As new research on Large Language Models (LLMs...   \n",
       "5  Recent image restoration methods have produced...   \n",
       "\n",
       "                                             summary  \n",
       "0  Recent advancements in artificial intelligence...  \n",
       "1  This research explores how to improve a proces...  \n",
       "2  Humans often discuss each other's beliefs, eve...  \n",
       "3  In many real-world situations, we encounter da...  \n",
       "4  As research on Large Language Models (LLMs) gr...  \n",
       "5  Recent advancements in image restoration have ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/abstract_pairs.parquet\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Total rows:\", len(df))\n",
    "\n",
    "# Keep up to 6800 examples\n",
    "if len(df) > 6800:\n",
    "    df = df.sample(6800, random_state=SEED).reset_index(drop=True)\n",
    "print(\"Using rows:\", len(df))\n",
    "\n",
    "df[[\"title\", \"input_text\", \"summary\"]].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da1ca7",
   "metadata": {},
   "source": [
    "### Build supervised prompts\n",
    "We create a single text field that includes:\n",
    "Instruction + Title + Abstract + `Output:` + gold summary\n",
    "\n",
    "The model is trained with next-token prediction on this sequence (SFT style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffe3ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "763   ### Instruction:\\nSummarize the following abst...   \n",
      "373   ### Instruction:\\nSummarize the following abst...   \n",
      "4836  ### Instruction:\\nSummarize the following abst...   \n",
      "2629  ### Instruction:\\nSummarize the following abst...   \n",
      "308   ### Instruction:\\nSummarize the following abst...   \n",
      "4100  ### Instruction:\\nSummarize the following abst...   \n",
      "\n",
      "                                                  title  \\\n",
      "763   Optimizing fairness tradeoffs in machine learn...   \n",
      "373   Intuition emerges in Maximum Caliber models at...   \n",
      "4836  Learning Computational Efficient Bots with Cos...   \n",
      "2629  A Nonlinear Hash-based Optimization Method for...   \n",
      "308   Boosting Theory-of-Mind Performance in Large L...   \n",
      "4100  Data Formulator: AI-powered Concept-driven Vis...   \n",
      "\n",
      "                                             input_text  \\\n",
      "763   Improving the fairness of machine learning mod...   \n",
      "373   Whether large predictive models merely parrot ...   \n",
      "4836  Deep reinforcement learning (DRL) techniques h...   \n",
      "2629  Sparse matrix-vector multiplication (SpMV) is ...   \n",
      "308   Large language models (LLMs) excel in many tas...   \n",
      "4100  With most modern visualization tools, authors ...   \n",
      "\n",
      "                                                summary  \n",
      "763   Researchers are working to make machine learni...  \n",
      "373   Researchers have explored how large predictive...  \n",
      "4836  Researchers have developed a new approach to i...  \n",
      "2629  Sparse matrix-vector multiplication (SpMV) is ...  \n",
      "308   A recent study explored how to improve the abi...  \n",
      "4100  Data Formulator is a new tool designed to make...  \n"
     ]
    }
   ],
   "source": [
    "def build_prompt(row):\n",
    "    return (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Summarize the following abstract into a clear, faithful 100–200 word summary for a general audience.\\n\\n\"\n",
    "        f\"### Title:\\n{row['title']}\\n\\n\"\n",
    "        f\"### Abstract:\\n{row['input_text']}\\n\\n\"\n",
    "        \"### Output:\\n\"\n",
    "        f\"{row['summary']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"text\"] = df.apply(build_prompt, axis=1)\n",
    "\n",
    "# Split into train/validation\n",
    "train_df, val_df = train_test_split(df[[\"text\", \"title\", \"input_text\", \"summary\"]],\n",
    "                                    test_size=0.1, random_state=SEED)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True)[[\"text\"]])\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True)[[\"text\"]])\n",
    "\n",
    "len(train_ds), len(val_ds)\n",
    "\n",
    "print(train_df.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c599b1",
   "metadata": {},
   "source": [
    "## 3) Tokenizer & model (4-bit with BitsAndBytes for QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d157ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130baf828756481c96ac32b6f69cd732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "SEQ_LEN = 4096  # safe starting point for RTX 3090 + 4-bit\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(HF_MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # good on Ampere (3090)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "model.config.use_cache = False  # for gradient checkpointing\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e888f3",
   "metadata": {},
   "source": [
    "## 4) LoRA config + SFTTrainer setup\n",
    "We use TRL's SFTTrainer to:\n",
    "- apply LoRA (PEFT)\n",
    "- handle packing and training loop\n",
    "- log loss / eval metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f12e721-41f2-4260-ae52-4202040215a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae8c4994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da4afc58ee54745918f4c4f3fe1dfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c26a19213d4f1eaa684e2c6d697f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6c996047a749e5ad544ac3035c7922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fead2c173504e03abec57b7049efb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18d034b7eeb41db8491af12c2364b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffdbe43dc304a47ae951f8423cb43da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import TrainingArguments  # still needed because SFTConfig inherits from it\n",
    "\n",
    "# LoRA config (same as before)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# All trainer hyperparameters now go into SFTConfig\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,      # effective batch size = 8\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=2,                 # start with 1 for smoke test, then 2–3\n",
    "    bf16=True,                          # or set to False/use fp16 if bf16 fails\n",
    "    logging_steps=25,                   # print train loss every 25 steps\n",
    "\n",
    "    # eval/save strategy names in recent transformers\n",
    "    eval_strategy=\"steps\",              # <— NOT evaluation_strategy\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",                   # change to \"wandb\" if you use W&B\n",
    "\n",
    "    # SFT-specific bits\n",
    "    dataset_text_field=\"text\",          # column name in train_ds / val_ds\n",
    "    max_length=SEQ_LEN,                 # <— use max_length, not max_seq_length\n",
    "    packing=False,                       # pack multiple examples per sequence\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    "    args=sft_config,        # pass the SFTConfig here\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tok,   # tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f927f1",
   "metadata": {},
   "source": [
    "## 5) Train and monitor progress\n",
    "Watch `loss` in the logs. If you enabled W&B, you'll get a live dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b51e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb71273",
   "metadata": {},
   "source": [
    "## 6) Save the LoRA adapter + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7960319",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"Adapter + tokenizer saved to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6bd76",
   "metadata": {},
   "source": [
    "## 7) Reload base vs fine-tuned and compare on a held-out subset\n",
    "We will:\n",
    "- Take ~200 examples from `val_df`\n",
    "- Prompt both base and fine-tuned models to summarize the abstract\n",
    "- Compute ROUGE-L and BERTScore-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import bert_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reload base model (same 4-bit config)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model.config.use_cache = True\n",
    "\n",
    "# Reload fine-tuned: base + LoRA adapter\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(ft_model, OUT_DIR)\n",
    "ft_model.config.use_cache = True\n",
    "\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tok, device_map=\"auto\")\n",
    "ft_pipe   = pipeline(\"text-generation\", model=ft_model,  tokenizer=tok, device_map=\"auto\")\n",
    "\n",
    "# Small validation subset for speed\n",
    "VAL_N = min(200, len(val_df))\n",
    "eval_slice = val_df.sample(VAL_N, random_state=SEED).reset_index(drop=True)\n",
    "print(\"Eval examples:\", len(eval_slice))\n",
    "\n",
    "def make_eval_prompt(abstract_text: str, title: str | None = None,\n",
    "                     min_w: int = 100, max_w: int = 200) -> str:\n",
    "    title_str = f\"Title: {title}\\n\\n\" if title else \"\"\n",
    "    return (\n",
    "        f\"Summarize the following abstract into a clear, faithful {min_w}–{max_w} word summary for a general audience.\\n\\n\"\n",
    "        f\"{title_str}\"\n",
    "        f\"Abstract:\\n{abstract_text}\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_summary(pipe, prompt: str, max_new: int = 300) -> str:\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "    )[0][\"generated_text\"]\n",
    "    # If model echoes the prompt, strip everything before \"Summary:\"\n",
    "    if \"Summary:\" in out:\n",
    "        out = out.split(\"Summary:\", 1)[-1]\n",
    "    return out.strip()\n",
    "\n",
    "\n",
    "refs = []\n",
    "base_preds = []\n",
    "ft_preds = []\n",
    "\n",
    "for _, row in tqdm(eval_slice.iterrows(), total=len(eval_slice), desc=\"Evaluating\"):\n",
    "    prompt = make_eval_prompt(row[\"input_text\"], row[\"title\"])\n",
    "    refs.append(row[\"summary\"])\n",
    "    base_preds.append(generate_summary(base_pipe, prompt))\n",
    "    ft_preds.append(generate_summary(ft_pipe, prompt))\n",
    "\n",
    "# ROUGE-L\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rg_base = rouge.compute(predictions=base_preds, references=refs, rouge_types=[\"rougeL\"])\n",
    "rg_ft   = rouge.compute(predictions=ft_preds,   references=refs, rouge_types=[\"rougeL\"])\n",
    "\n",
    "# BERTScore (F1)\n",
    "P_b, R_b, F_b = bert_score.score(base_preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "P_f, R_f, F_f = bert_score.score(ft_preds,   refs, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "print(\"\\n=== ROUGE-L ===\")\n",
    "print(\"Base      :\", rg_base[\"rougeL\"])\n",
    "print(\"Fine-tuned:\", rg_ft[\"rougeL\"])\n",
    "\n",
    "print(\"\\n=== BERTScore-F1 (mean) ===\")\n",
    "print(\"Base      :\", float(F_b.mean()))\n",
    "print(\"Fine-tuned:\", float(F_f.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eae6f9",
   "metadata": {},
   "source": [
    "## 8) Helper: load fine-tuned pipeline later and run a test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_pipeline(adapter_dir: str | Path = OUT_DIR):\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        HF_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    ft = PeftModel.from_pretrained(base, adapter_dir)\n",
    "    ft.config.use_cache = True\n",
    "    return pipeline(\"text-generation\", model=ft, tokenizer=tok, device_map=\"auto\")\n",
    "\n",
    "\n",
    "ft_pipe = load_finetuned_pipeline()\n",
    "\n",
    "test_abstract = \"\"\"Transformers use self-attention to capture long-range dependencies in sequences.\n",
    "However, the quadratic complexity of standard attention limits practicality for very long inputs.\n",
    "We propose a sparse attention mechanism that preserves performance while reducing computational cost,\n",
    "and demonstrate improvements on language modeling and long-document summarization benchmarks.\"\"\"\n",
    "\n",
    "test_title = \"Efficient Sparse Attention for Long-Context Transformers\"\n",
    "\n",
    "test_prompt = make_eval_prompt(test_abstract, test_title)\n",
    "print(\"\\n--- Fine-tuned model summary ---\\n\")\n",
    "print(ft_pipe(test_prompt, max_new_tokens=280, do_sample=False, temperature=0.0)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858425b",
   "metadata": {},
   "source": [
    "## 9) (Optional) Merge LoRA into full weights (for export)\n",
    "This step is optional and more VRAM-heavy. Usually you can just keep LoRA separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "# merged = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     OUT_DIR,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# ).merge_and_unload()\n",
    "# merged_dir = OUT_DIR / \"merged_fp16\"\n",
    "# merged.save_pretrained(merged_dir)\n",
    "# tok.save_pretrained(merged_dir)\n",
    "# print(\"Merged full model saved to:\", merged_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchPaperFineTune",
   "language": "python",
   "name": "researchpaperfinetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
